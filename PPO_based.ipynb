{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/eGrocery_Demand_Prediction/blob/main/PPO_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhMmWHO661qS"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iTM_JM0l7Vp"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import MultivariateNormal\n",
        "import random\n",
        "import pandas as pd\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K2C6nkGl7Vq"
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWm2VI2Yl7Vr"
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_std, device):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\t#actor\n",
        "        self.actor =  nn.Sequential(\n",
        "                nn.Linear(state_dim, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, action_dim),\n",
        "\t\tnn.ReLU()\n",
        "                )\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(state_dim, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 1)\n",
        "                )\n",
        "        self.device = device\n",
        "        self.action_var = torch.full((action_dim,), action_std*action_std).to(self.device)\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, memory):\n",
        "        action_mean = self.actor(state)\n",
        "        cov_mat = torch.diag(self.action_var).to(self.device)\n",
        "\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(action_logprob)\n",
        "\n",
        "        return action.detach()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_mean = self.actor(state)\n",
        "\n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var).to(self.device)\n",
        "\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_value = self.critic(state)\n",
        "\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4u_QxiGl7Vs"
      },
      "source": [
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip, device):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        self.device = device\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, action_std,self.device).to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, action_std,self.device).to(self.device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state, memory):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
        "        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo estimate of rewards:\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards:\n",
        "        rewards = torch.tensor(rewards).to(self.device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(memory.states).to(self.device), 1).detach()\n",
        "        old_actions = torch.squeeze(torch.stack(memory.actions).to(self.device), 1).detach()\n",
        "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs), 1).to(self.device).detach()\n",
        "\n",
        "        # Optimize policy for K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # Evaluating old actions and values :\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old):\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss:\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "            loss = loss.type(torch.cuda.FloatTensor)\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy:\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yx9xf2fl7Vu"
      },
      "source": [
        "class EGroceryEnv():\n",
        "\n",
        "\tdef __init__(self, df=pd.DataFrame({0:[0]}), products_count=10, features=['a'], shelf_life=[1], wastage_cost=[1], shortage_cost=[1]):\n",
        "\t\tsuper(EGroceryEnv, self).__init__()\n",
        "\n",
        "\t\tself.df = df\n",
        "\t\tself.products_count = products_count\n",
        "\t\tself.shelf_life = shelf_life\n",
        "\t\tself.features = features\n",
        "\t\tself.wastage_cost = wastage_cost\n",
        "\t\tself.shortage_cost = shortage_cost\n",
        "\t\tself.current_step = 0\n",
        "\t\tself.wastage_track = list([])\n",
        "\t\tself.shortage_track = list([])\n",
        "\t\tself.reward_track = list([])\n",
        "\n",
        "\t\t#variables to track shartage and wastage\n",
        "\t\tself.shortage = np.array(list([0]*self.products_count))\n",
        "\t\tself.wastage = np.array(list([0]*self.products_count))\n",
        "\n",
        "\t\t#Define Stock\n",
        "\t\tself.stock = list([])\n",
        "\t\tfor i in range(self.products_count):\n",
        "\t\t\tself.stock.append([0])\n",
        "\n",
        "\n",
        "\tdef _next_observation(self):\n",
        "\n",
        "\t\tobs  = self.df.loc[self.current_step,self.features]\n",
        "\n",
        "\t\tst_temp = list([])\n",
        "\t\tfor i in range(len(self.stock)):\n",
        "\t\t\tfor j in range(1,min(int(self.stock[i][0])+1,5)):\n",
        "\t\t\t\tst_temp.append(self.stock[i][j])\n",
        "\t\t\tif(self.stock[i][0]==5):\n",
        "\t\t\t\tst_temp.append(self.stock[i][4])\n",
        "\t\t\telif(self.stock[i][0]<5):\n",
        "\t\t\t\tfor j in range(int(self.stock[i][0])+1,6):\n",
        "\t\t\t\t\tst_temp.append(0)\n",
        "\t\t\telse:\n",
        "\t\t\t\tst_temp.append(np.sum(self.stock[i][5:int(self.stock[i][0])+1]))\n",
        "\t\tobs = list(obs) + list(st_temp) + list(self.shelf_life)\n",
        "\t\treturn obs\n",
        "\n",
        "\tdef _take_action(self, action):\n",
        "\t\t#Add products to the current stocks\n",
        "\t\tfor i in range(self.products_count):\n",
        "\t\t\tif(len(self.stock[i])<self.shelf_life[i]):\n",
        "\t\t\t\tfor j in range(len(self.stock[i]),int(self.shelf_life[i])):\n",
        "\t\t\t\t\tself.stock[i].append(0)\n",
        "\t\t\tself.stock[i].append(action[i])\n",
        "\t\t\tself.stock[i][0]=self.shelf_life[i]\n",
        "\n",
        "\n",
        "\n",
        "\t\t#Fullfill demand\n",
        "\t\tprods = ['prod'+str(i) for i in [8,11,15,17,94,95,96,110,112,128]]\n",
        "\t\tdemand = self.df.loc[self.current_step+1,prods]\n",
        "\t\tfor i in range(self.products_count):\n",
        "\t\t\tfor j in range(1,int(self.stock[i][0])+1):\n",
        "\t\t\t\tif(self.stock[i][j]>=demand[i]):\n",
        "\t\t\t\t\tself.stock[i][j] = self.stock[i][j] - demand[i]\n",
        "\t\t\t\t\tdemand[i] = 0\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tdemand[i] = demand[i] - self.stock[i][j]\n",
        "\t\t\t\t\tself.stock[i][j] = 0\n",
        "\t\t\tif(demand[i]>0):\n",
        "\t\t\t\tself.shortage[i]=demand[i]\n",
        "\n",
        "\t\t#Update shelf life and find out wastage\n",
        "\t\tfor i in range(self.products_count):\n",
        "\t\t\tself.stock[i][0] = self.stock[i][0] -1\n",
        "\t\t\tif(self.stock[i][1]>0):\n",
        "\t\t\t\tself.wastage[i] = self.stock[i][1]\n",
        "\t\t\tfor j in range(1,int(self.stock[i][0])+1):\n",
        "\t\t\t\tself.stock[i][j] = self.stock[i][j+1]\n",
        "\t\t\tself.stock[i].pop()\n",
        "\n",
        "\n",
        "\tdef step(self, action):\n",
        "\t        # update stock, fullfill demand and calculate shortage and wastage\n",
        "\t\tquantity = [6, 10, 15, 4, 6, 2, 7, 50, 2, 30]\n",
        "\t\taction1 = [0]*self.products_count\n",
        "\t\tfor i in range(len(action)):\n",
        "\t\t\taction1[i] = action[i]*quantity[i]\n",
        "\t\tself._take_action(action1)\n",
        "\t\tself.action = action\n",
        "\n",
        "\t\t#increment step\n",
        "\t\tself.current_step += 1\n",
        "\n",
        "\n",
        "\t\treward = -1*(np.matmul(self.wastage_cost,self.wastage.transpose())+np.matmul(self.shortage_cost,self.shortage.transpose()))\n",
        "\t\tself.reward = reward\n",
        "\t\tdone = (self.current_step < 0) or (self.current_step > self.df.shape[0]-2)\n",
        "\n",
        "\t\tobs = self._next_observation()\n",
        "\n",
        "\t\tself.wastage_track.append(np.sum(self.wastage))\n",
        "\t\tself.shortage_track.append(np.sum(self.shortage))\n",
        "\t\tself.reward_track.append(np.abs(self.reward))\n",
        "\n",
        "\n",
        "\t\tself.shortage = np.array(list([0]*self.products_count))\n",
        "\t\tself.wastage = np.array(list([0]*self.products_count))\n",
        "\n",
        "\t\treturn obs, reward, done, {}\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\t# Reset the state of the environment to an initial state\n",
        "\t\tself.current_step = 0\n",
        "\t\tself.shortage = np.array(list([0]*self.products_count))\n",
        "\t\tself.wastage = np.array(list([0]*self.products_count))\n",
        "\t\tself.stock = list([])\n",
        "\t\tfor i in range(self.products_count):\n",
        "\t\t\tself.stock.append([0])\n",
        "\n",
        "\t\treturn [0]*len(self.features) + [0]*6*self.products_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiA8zXcnl7Vz"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHZRDQYNl7V3",
        "outputId": "0d8ed1c1-6b5e-4d6d-dacc-8a531a720141"
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/RL_Project/Models/ppo_based/PPOBased/data/final_data_trainx.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/RL_Project/Models/ppo_based/PPOBased/data/final_data_testx.csv')\n",
        "\n",
        "products_count = 10\n",
        "\n",
        "avg_f7 = ['prod'+str(i)+'avg7' for i in [8,11,15,17,94,95,96,110,112,128]]\n",
        "avg_f15 = ['prod'+str(i)+'avg15' for i in [8,11,15,17,94,95,96,110,112,128]]\n",
        "avg_f30 = ['prod'+str(i)+'avg30' for i in [8,11,15,17,94,95,96,110,112,128]]\n",
        "\n",
        "features = ['month', 'monthday', 'weekday'] + avg_f7 + avg_f15 + avg_f30\n",
        "\n",
        "print(features)\n",
        "\n",
        "shelf_life = np.array([4, 3, 5, 10, 7, 2, 1, 3, 8, 6], dtype=np.float32)\n",
        "\n",
        "wastage_cost = np.array([1]*products_count, dtype=np.float16)\n",
        "shortage_cost = np.array([1]*products_count, dtype=np.float16)\n",
        "\n",
        "action_std = 0.1\n",
        "eps_clip = 0.2\n",
        "gamma = 0.99\n",
        "\n",
        "lr = 0.00001\n",
        "betas = (0.9, 0.999)\n",
        "K_epochs = 5\n",
        "\n",
        "update_timestep = 100\n",
        "time_step=0\n",
        "running_reward = 0\n",
        "\n",
        "state_dim = len(features) + 6*products_count\n",
        "action_dim = products_count\n",
        "\n",
        "env = EGroceryEnv(df_train, products_count, features, shelf_life, wastage_cost, shortage_cost)\n",
        "\n",
        "memory = Memory()\n",
        "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip, device)\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "Total_reward = []\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['month', 'monthday', 'weekday', 'prod8avg7', 'prod11avg7', 'prod15avg7', 'prod17avg7', 'prod94avg7', 'prod95avg7', 'prod96avg7', 'prod110avg7', 'prod112avg7', 'prod128avg7', 'prod8avg15', 'prod11avg15', 'prod15avg15', 'prod17avg15', 'prod94avg15', 'prod95avg15', 'prod96avg15', 'prod110avg15', 'prod112avg15', 'prod128avg15', 'prod8avg30', 'prod11avg30', 'prod15avg30', 'prod17avg30', 'prod94avg30', 'prod95avg30', 'prod96avg30', 'prod110avg30', 'prod112avg30', 'prod128avg30']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYcfLkXKETEf",
        "outputId": "71579c97-c6e4-44c0-c446-f4afdf886e47"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDqP6ccal7V4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5d9a42-04b6-4644-bcb7-2aca3e74ecca"
      },
      "source": [
        "for epoch in range(1,5):\n",
        "    obs = env.reset()\n",
        "    obs = np.array(obs)\n",
        "    running_reward = 0\n",
        "\n",
        "    for i in range(50):\n",
        "        time_step +=1\n",
        "\n",
        "        action = ppo.select_action(obs, memory)\n",
        "        action = action\n",
        "        action = list(action.astype(int))\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        obs = np.array(obs)\n",
        "\n",
        "        memory.rewards.append(reward)\n",
        "        memory.is_terminals.append(done)\n",
        "\n",
        "        if(time_step%update_timestep==0):\n",
        "            ppo.update(memory)\n",
        "            memory.clear_memory()\n",
        "            timestep = 0\n",
        "        running_reward+=reward\n",
        "\n",
        "        writer.add_scalar(\"reward\", reward, time_step)\n",
        "\n",
        "        if(done==1):\n",
        "            break\n",
        "\n",
        "    Total_reward.append(running_reward)\n",
        "\n",
        "\n",
        "\n",
        "    if(epoch%1)==0:\n",
        "        torch.save(ppo.policy.state_dict(), '/content/drive/MyDrive/Colab Notebooks/RL_Project/Models/ppo_based/PPOBased/model_{}.pth'.format(epoch))\n",
        "        pd.DataFrame({'Epoch':epoch,'Reward ': Total_reward}).to_csv('/content/drive/MyDrive/Colab Notebooks/RL_Project/Models/ppo_based/PPOBased/runs/Training.csv',index=False)\n",
        "\n",
        "    print('Epoch ', epoch, ' Reward ', running_reward)\n",
        "\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1  Reward  -15763.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}